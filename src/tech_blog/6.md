I have been coding since I was a freshman in college (2016) but it took me this long (9 years) to make my first open source contribution.

The repo is [apache/arrow-rs](https://github.com/apache/arrow-rs) (pure rust implementation of Apache Arrow). I had some time on my hands and wanted to enhance my Rust skills by working on a large and widely used open source project. I have tried to make pull requests to open source projects before, but I was either met with resistance, ghosting, immediate closure with a vague reason, or what almost seems like an intentional misunderstanding of my contribution. 

I wanted to be mature this time. I had time to make a high quality contribution, learn the code base sufficiently enough, explain my solution so that it is hard to misunderstand it, and make the contribution so mouth wattering that the maintainers would be begging to merge it.

Easier said than done. Here is my story of issue [#7273](https://github.com/apache/arrow-rs/issues/7273)

## Repository Selection
So which repo should I choose? Well there are a lot of repos out there that have maintainers who have a big ego, either they are part of a company and their only priority is to make money by adding features and not look at technical debt clean up contributions, or just contributors who get mad when you point out an issue with their "perfect" code.

What I did was ask for a recommendation. Luckilly I was able to talk to [Denny Lee](https://www.linkedin.com/in/dennyglee/) the Director of Development Relations at Databricks, at a [PyData Seattle meet up](https://www.meetup.com/bellevue-gdg). He was speeking about all the open source libraries Databricks makes use of, some of which are implemented in Rust. I went up to him after the presentation and asked what open source projects have a good culture behind them that I may be able to contribute some code to? He said so many that I could not keep up, but one of them I remembered was Apache Arrow.

So I looked up Apache Arrow, and saw that it was not a Rust repository, it was written in C. It was also hosted and managed by isoteric tooling (not GitHub). I got sad, until I asked an LLM if it knew a Rust version of the repo, and it pointed me directly there (Why is SEO so bad now adays?). Once I found it, I was elated. I felt like I was living on a prayer enough so that I was determined to make this repo be the one to contribute to.

## Issue Selection
I first took a look at the activity of the repo. I saw contributions from hundreds of contributors within the past year (a good sign). I saw that more than one person was approving pull requests. I saw that on many issues, there was multiple people weighing in on the correct implementation, and it seemed like a cordial discussion (a good sign).

Now how do I even contribute? I dont want to have to sign up for some account on another website just to sign a contributor agreement or something. Also like how do you know that you are the only one working on an issue at a time so your effort is not wasted? Are there different levels of contributors that can touch different segments of the code and if I am not at that level yet, would my contribution be laughed at?

Luckilly in the README it gives instructions to new contributors of the codebase. After reading it, I was supprised that I understood just about everything that was explained there. No dogmatic Makefiles, no hacked testing/benching system, just the standard Rust tooling which I was already comfortable with.

The README also talked about how there was an issue tag called `good first issue` where maintainers would tag such that newer contributors could pick up easily. I looked at those tags, and was surprised to find that there were only 20, and most of them had either a bunch of discussion that came to the conclusion that the solution was too hard or infeasable, or I had no idea what the issue was talking about

If only there was an issue that someone explained really well, was a one or two line change, that had almost no testing/benching complexity, that you only needed to understand one file to understand the scope of the fix, and that had a link to code that already implemented and tested a fix for (so I did not need to do it myself). Well, you are in luck because I found an issue exactly matching that description.

Welcome to issue [#7273](https://github.com/apache/arrow-rs/issues/7273). The issue wanted the integer/float to string conversion to be faster, by using no extra copies. I actually understand what that means! This issue was actually created by one of the maintainers which is a good sign, maybe he will actually look at my contribution. There was a proposed solution by the maintainer, and a contributor already implemented a solution and a benchmark, but unfortunately the new solution benched slower than what was there, so they gave up, but they linked to their branch they committed code to.

## Implementation
I was excited, I had some working code and a benchmark to work with, so I checked out the code and ran the bench and saw it was running slower, and was perplexed since it did elimitate one copy. The benchmark seemed like a good test case, but then the issue stood out to me. Was the benchmark even testing the correct code path? A simple print debugging test revealed that it was not. Then I changed the benchmark to test the correct code path, and I saw a 50 percent improvement in runtime!

I figured I could just fix the benchmark and call it done, but there was some extra credit put in the description of the issue. We could possibly use an external library to make the runtime even faster. I added this and saw that the runtime was truly even faster! a full half reduction in runtime speed across the corrected benchmarks. I was even more elated, so I submitted my code and tagged the creator of the issue to ask for a review

## Maintainer Argument
To my surprise, the maintainer acknowledged my submission the next day and ran some automated benchmark on the code, and all the code related to conversions regressed by 15 percent. What happened? Also I saw his automated benchmark did not include my new benchmark. He said he would not merge it because it regressed. I had to run the benchmarks myself to see them for my eyes that they regresses, and he was correct, other types of conversions were slower. That made no sense to me since I did not change that code path. The only thing I could think of that changed was some compiler optimization was able to run when the code around other conversions were simpler and it was not able to run if the file contained more complex logic? I guess it was possible, but I did not know compile internals enough to prove it.

I asked the maintainer why that if there was a 15 percent regression in other data types and a 100% speed up of integer and float conversions that that would not be grounds for inclusion anyway? He ignored me. I was mad. I was feeling this was turning into one of those things where even though it was the best for the project, the maintainer could still block it because he has the power and he uses it however he feels like it (he has a soft spot for strings over integers?)

## Another idea
3 months passed (3 months!). Still mad, I was reading that the Rust compiler only does vectorization optimizations for x86_64 CPUs that were built in the year 2000. There were a lot of vector instruction improvements to the ISA since then, which are included in newer CPUs, which I assumed most Apache Arrow RS code was running on. The only issue is that you have to run the Rust compiler with a flag that tells it to optimize for the newest vectorization instructions.

I just wanted to test for myself to see if benchmarking the code I wrote to speed up int to string conversions would test faster with newer vectorization hardware. So I did, I created a benchmark with and without the vectorization flag, and found something unexpected. the int/float to string conversions were not faster, but there was something that did not change that suprised me. The other types conversion to string code seemed to not regress anymore with the new vector instructions even with the optimizations I wrote for the int?float types.

This gives me some sort of proof that I may not be the cause of the regression and it was the compiler choosing things. I knew it was a long shot but I tagged the maintainer again and asked him to run another benchmark against my change with the different compiler flag, and the results shocked me for an even different reason. The maintainer was actually excited to do that and did it a day later. The benchmarks actually turned out to not regress with the compiler flag turned on as well as turned off!

That makes no sense? How can something regress significantly one time but not regress 3 months later with the same code? The only answer I could think of was that since the code base was configured to use a newer Rust version to compile the code, maybe the same compiler optimization used to optimize the code before I touched it was finally changed to run on my implemented int/float optimization logic too. 

Anyway the maintainer immediately saw that there were no regressions like before and that there was a 100% speed up of int/float conversions, and he had no choice but to drool and merge the code! The code was merged on Sep 29 2025!